{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## To run this file\n",
    "\n",
    "### Meta tokenizer\n",
    "\n",
    "First, copy and paste the original meta `tokenizer.py` file from [here](https://github.com/facebookresearch/llama/blob/6c7fe276574e78057f917549435a2554000a876d/llama/tokenizer.py).\n",
    "Also, you'll need the original `tokenizer.model` file. For that, get the download [link](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and download it following the [instructions](https://github.com/facebookresearch/llama/blob/6c7fe276574e78057f917549435a2554000a876d/download.sh#L20). Place it inside the `models` folder.\n",
    "\n",
    "### llama-cpp-python model\n",
    "\n",
    "I'm using the [The Bloke](https://huggingface.co/TheBloke/Llama-2-7B-GGML) models. Go to the link and download the model `llama-2-7b-chat.ggmlv3.q2_K.bin` which is the smallest one. Place it inside the `models` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_cpp import Llama\n",
    "from typing import List\n",
    "from chat_messages_formatter import Message, llama2_format_messages\n",
    "from llama_cpp_wrapper import llama_cpp_tokenizer_encode\n",
    "from tokenizer import Tokenizer\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llama_cpp_llm = Llama(model_path=os.path.join(\"models\", \"llama-2-7b-chat.ggmlv3.q2_K.bin\"))\n",
    "meta_tokenizer = Tokenizer(model_path=os.path.join(\"models\", \"tokenizer.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages: List[Message] = [\n",
    "        Message(role=\"user\", content=\"How are you?\"),\n",
    "        Message(role=\"assistant\", content=\"I'm fine!\"),\n",
    "        Message(role=\"user\", content=\"Write a four page long essay about Hawaii.\"),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 29961, 25580, 29962, 3532, 14816, 29903, 6778, 13, 3492, 526, 263, 8444, 29892, 3390, 1319, 322, 15993, 20255, 29889, 29849, 1234, 408, 1371, 3730, 408, 1950, 29892, 1550, 1641, 9109, 29889, 3575, 6089, 881, 451, 3160, 738, 10311, 1319, 29892, 443, 621, 936, 29892, 11021, 391, 29892, 7916, 391, 29892, 304, 27375, 29892, 18215, 29892, 470, 27302, 2793, 29889, 3529, 9801, 393, 596, 20890, 526, 5374, 635, 443, 5365, 1463, 322, 6374, 297, 5469, 29889, 13, 13, 3644, 263, 1139, 947, 451, 1207, 738, 4060, 29892, 470, 338, 451, 2114, 1474, 16165, 261, 296, 29892, 5649, 2020, 2012, 310, 22862, 1554, 451, 1959, 29889, 960, 366, 1016, 29915, 29873, 1073, 278, 1234, 304, 263, 1139, 29892, 3113, 1016, 29915, 29873, 6232, 2089, 2472, 29889, 13, 29966, 829, 14816, 29903, 6778, 13, 13, 5328, 526, 366, 29973, 518, 29914, 25580, 29962, 306, 29915, 29885, 2691, 29991, 29871, 2, 1, 29961, 25580, 29962, 14350, 263, 3023, 1813, 1472, 3686, 388, 1048, 26901, 29875, 29889, 518, 29914, 25580, 29962]\n"
     ]
    }
   ],
   "source": [
    "llama_cpp_messages_tokens = llama2_format_messages(messages, tokenizer_encode=partial(llama_cpp_tokenizer_encode, llm=llama_cpp_llm))\n",
    "print(llama_cpp_messages_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 518, 25580, 29962, 3532, 14816, 29903, 6778, 13, 3492, 526, 263, 8444, 29892, 3390, 1319, 322, 15993, 20255, 29889, 29849, 1234, 408, 1371, 3730, 408, 1950, 29892, 1550, 1641, 9109, 29889, 3575, 6089, 881, 451, 3160, 738, 10311, 1319, 29892, 443, 621, 936, 29892, 11021, 391, 29892, 7916, 391, 29892, 304, 27375, 29892, 18215, 29892, 470, 27302, 2793, 29889, 3529, 9801, 393, 596, 20890, 526, 5374, 635, 443, 5365, 1463, 322, 6374, 297, 5469, 29889, 13, 13, 3644, 263, 1139, 947, 451, 1207, 738, 4060, 29892, 470, 338, 451, 2114, 1474, 16165, 261, 296, 29892, 5649, 2020, 2012, 310, 22862, 1554, 451, 1959, 29889, 960, 366, 1016, 29915, 29873, 1073, 278, 1234, 304, 263, 1139, 29892, 3113, 1016, 29915, 29873, 6232, 2089, 2472, 29889, 13, 29966, 829, 14816, 29903, 6778, 13, 13, 5328, 526, 366, 29973, 518, 29914, 25580, 29962, 306, 29915, 29885, 2691, 29991, 29871, 2, 1, 518, 25580, 29962, 14350, 263, 3023, 1813, 1472, 3686, 388, 1048, 26901, 29875, 29889, 518, 29914, 25580, 29962]\n"
     ]
    }
   ],
   "source": [
    "meta_messages_tokens = llama2_format_messages(messages, tokenizer_encode=meta_tokenizer.encode)\n",
    "print(meta_messages_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, the difference is between the tokens `518` and `29961`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  1 518 != 29961\n",
      "#149 518 != 29961\n"
     ]
    }
   ],
   "source": [
    "assert len(meta_messages_tokens) == len(llama_cpp_messages_tokens)\n",
    "\n",
    "for i, (a, b) in enumerate(zip(meta_messages_tokens, llama_cpp_messages_tokens)):\n",
    "    if a != b:\n",
    "        print(f\"#{i:3d}\", a, \"!=\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm comparing the decodification for the tokens `518` and `29961`. I also added the token `29871` because it'll appear later in this notebook. Added `|` as a delimiter to ease visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detokenizing token 518 with llama_cpp_llm              : type |<class 'bytes'>| length |2| detokenized |b' ['|\n",
      "Detokenizing token 518 with llama_cpp_llm and decoding : type |<class 'str'>| length |2| detokenized | [|\n",
      "Detokenizing token 518 with meta_tokenizer             : type |<class 'str'>| length |1| detokenized |[|\n",
      "\n",
      "Detokenizing token 29961 with llama_cpp_llm              : type |<class 'bytes'>| length |1| detokenized |b'['|\n",
      "Detokenizing token 29961 with llama_cpp_llm and decoding : type |<class 'str'>| length |1| detokenized |[|\n",
      "Detokenizing token 29961 with meta_tokenizer             : type |<class 'str'>| length |1| detokenized |[|\n",
      "\n",
      "Detokenizing token 29871 with llama_cpp_llm              : type |<class 'bytes'>| length |1| detokenized |b' '|\n",
      "Detokenizing token 29871 with llama_cpp_llm and decoding : type |<class 'str'>| length |1| detokenized | |\n",
      "Detokenizing token 29871 with meta_tokenizer             : type |<class 'str'>| length |0| detokenized ||\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in [518, 29961, 29871]:\n",
    "    ld = llama_cpp_llm.detokenize([token])\n",
    "    decoded_ld = ld.decode('utf-8')\n",
    "    md = meta_tokenizer.decode([token])\n",
    "    print(f\"Detokenizing token {token} with llama_cpp_llm              : type |{type(ld)}| length |{len(ld)}| detokenized |{ld}|\")\n",
    "    print(f\"Detokenizing token {token} with llama_cpp_llm and decoding : type |{type(decoded_ld)}| length |{len(decoded_ld)}| detokenized |{decoded_ld}|\")\n",
    "    print(f\"Detokenizing token {token} with meta_tokenizer             : type |{type(md)}| length |{len(md)}| detokenized |{md}|\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm comparing the codification for the texts decoded above, which are |` [`| and |`[`|. Added `|` as a delimiter to ease visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text | [| with encoding + llama_cpp_llm   : type |<class 'list'>| length |1| tokenized |[518]|\n",
      "Tokenizing text | [| with llama_cpp_tokenizer_encode : type |<class 'list'>| length |1| tokenized |[518]|\n",
      "Tokenizing text | [| with meta_tokenizer             : type |<class 'list'>| length |2| tokenized |[29871, 518]|\n",
      "\n",
      "Tokenizing text |[| with encoding + llama_cpp_llm   : type |<class 'list'>| length |1| tokenized |[29961]|\n",
      "Tokenizing text |[| with llama_cpp_tokenizer_encode : type |<class 'list'>| length |1| tokenized |[29961]|\n",
      "Tokenizing text |[| with meta_tokenizer             : type |<class 'list'>| length |1| tokenized |[518]|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in [\" [\", \"[\"]:\n",
    "    le = llama_cpp_llm.tokenize(text.encode('utf-8'), add_bos=False)\n",
    "    lte = llama_cpp_tokenizer_encode(text, bos=False, eos=False, llm=llama_cpp_llm)\n",
    "    me = meta_tokenizer.encode(text, bos=False, eos=False)\n",
    "    print(f\"Tokenizing text |{text}| with encoding + llama_cpp_llm   : type |{type(le)}| length |{len(le)}| tokenized |{le}|\")\n",
    "    print(f\"Tokenizing text |{text}| with llama_cpp_tokenizer_encode : type |{type(lte)}| length |{len(lte)}| tokenized |{lte}|\")\n",
    "    print(f\"Tokenizing text |{text}| with meta_tokenizer             : type |{type(me)}| length |{len(me)}| tokenized |{me}|\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
